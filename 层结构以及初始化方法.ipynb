{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 层结构以及初始化方法\n",
    "## 1、卷积层\n",
    "<img src=\"/mnt/mydisk2/myPytorch/pic/卷积层.png\" alt=\"FAO\" width=\"790\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1))\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "conv = nn.Conv2d(3,64,3,1,0)\n",
    "print(conv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算输出特征图的大小\n",
    "- 假设当前图像的大小为：Hin*Win\n",
    "- 假设卷积核大小为: FH * FW ;个数为FN\n",
    "- 填充数(padding)为P，步长 (stride)为S\n",
    "- 输出图像的大小为: Hout * Wout\n",
    "- 输出图像维度为:(FN, Hout, Wout)\n",
    "- 其中Padding如果取VALID模式，则p=0;如果取SAME,则p>0\n",
    "\n",
    "i) 不含膨胀率的计算方法:  \n",
    "Hout = (Hin + 2*P - FH) / S + 1  \n",
    "Wout = (Win + 2*P - FH) / S + 1  \n",
    "ii) 含膨胀率的计算方法:  \n",
    "Hout = (Hin + 2*P - D*(FH-1) - 1) / S + 1  \n",
    "Wout = (Win + 2*P - D*(FH-1) - 1) / S + 1  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反卷积\n",
    "#### 反卷积就是卷积的反向操作\n",
    "<img src=\"/mnt/mydisk2/myPytorch/pic/反卷积演示.png\" alt=\"FAO\" width=\"790\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "反卷积的两种方法：  \n",
    "1、双线性插值上采样  \n",
    "<img src=\"/mnt/mydisk2/myPytorch/pic/反卷积_双线性插值上采样.png\" alt=\"FAO\" width=\"790\">\n",
    "\n",
    "`bilinear_layer = nn.UpsamplingBilinear2d(size=None,scale_factor=None)`  \n",
    "size表示期望的输出尺寸，scale_factor表示缩放因子，用来决定缩放的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 28, 28])\n",
      "torch.Size([1, 16, 14, 14])\n",
      "torch.Size([1, 16, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 定义一个卷积层，用来提取图像特征\n",
    "conv = nn.Conv2d(1, 16, kernel_size=3, stride=2, padding=1)\n",
    "# 假设输入的图像大小是 28x28，经过卷积层后，得到一个 14x14 的特征图\n",
    "x = torch.randn(1, 1, 28, 28)\n",
    "print(x.shape)\n",
    "x = conv(x)\n",
    "print(x.shape)\n",
    "\n",
    "# 定义一个上采样模块，用来恢复特征图大小\n",
    "up_bilinear = nn.UpsamplingBilinear2d(size=(56,56))\n",
    "x = up_bilinear(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2、转置卷积  \n",
    "（1）转置卷积通过学习的方式，即通过在训练中更新卷积核的参数，以完成上采样过程  \n",
    "（2）其计算结果往往更具鲁棒性  \n",
    "（3）缺点是会增加模型的训练时间和训练参数  \n",
    "（4）其具体代码与卷积代码类似：比卷积层仅多了一个输出填充参数，其他参数均不变  \n",
    "`transpose_conv = nn.ConvTranspose2d(in_channels,out_channels,kernel_size,stride=1,padding=0,output_padding=0,group=1,bias=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2、池化层\n",
    "<img src=\"/mnt/mydisk2/myPytorch/pic/池化层.png\" alt=\"FAO\" width=\"790\">  \n",
    " \n",
    "1、最大池化\n",
    "\n",
    "`\n",
    "maxpool_layer = nn.MaxPool2d(kernel_size,stride=None,padding=0,dilation=1,return_indices=False,ceil_mode=False)\n",
    "`\n",
    "\n",
    "2、平均池化  \n",
    "\n",
    "`average_layer = nn.AvgPool2d(kernel_size,stride=None,padding=0,ceil_mode=False,count_include_pad=True)`\n",
    "\n",
    "- kernel_size(int or tuple) - max pooling的窗口大小  \n",
    "- stride(int or tuple, optional) - max pooling的窗口移动的步长。默认值是- kernel_size\n",
    "- padding(int or tuple, optional) - 输入的每一条边补充0的层数\n",
    "- dilation(int or tuple, optional) – 一个控制窗口中元素步幅的参数\n",
    "- return_indices - 如果等于True，会返回输出最大值的序号，对于上采样操作会有帮助\n",
    "- ceil_mode - 如果等于True，计算输出信号大小的时候，会使用向上取整，代替默认的向下取整的操作\n",
    "\n",
    "3、Mixed Pooling（对固定区域随机最大和平均池化）  \n",
    "\n",
    "4、Stochastic Pooling（对随机区域随机最大和平均池化）  \n",
    "\n",
    "<u>采用平均池化和最大池化最终一般会得到类似的结果</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3、正则化层\n",
    "<img src=\"/mnt/mydisk2/myPytorch/pic/正则化层.png\" alt=\"FAO\" width=\"790\"> \n",
    "\n",
    "&emsp;&emsp;正则化层是神经网络中非常重要的操作之一其全称为Batch它通过将数据进Normalization (BN)，也就是标准化外理。行偏移和尺度缩放调整，在数据预处理时是非常常见的操作，在网络的中间层如今也很频繁的被使用。\n",
    "- 完成对数据的便宜和缩放，使其保持原有的分布特征，从而补偿网络的非线性表达能力损失\n",
    "- 好处：  \n",
    "（1）减轻对初始数据的依赖  \n",
    "（2）加速训练，学习率可以设置的更高\n",
    "- 坏处：  \n",
    "依赖batch的大小，batch不同，方差和均值的计算不稳定，导致BN不适合batch较小的场景，不适合RNN\n",
    "\n",
    "`BN = nn.BatchNorm2d(channel)`\n",
    "\n",
    "# 4、激活函数\n",
    "<img src=\"/mnt/mydisk2/myPytorch/pic/激活函数.png\" alt=\"FAO\" width=\"790\"> \n",
    "\n",
    "- 在实际应用中，我们用神经网络期望解决的问题往往是非线性的，这就需要我们引入更多的非线性因素，而激活函数便承担了此功能。\n",
    "\n",
    "`relu = nn.ReLU()`\n",
    "\n",
    "# 5、全连接层\n",
    "<img src=\"/mnt/mydisk2/myPytorch/pic/全连接层.png\" alt=\"FAO\" width=\"790\"> \n",
    "\n",
    "全连接层是卷积神经网络中重要的组成部分之一，起到“分类器”的作用全连接层的作用，是将特征图的分布式表示映射到样本标记空间。通俗来讲，就是将某一特征图的表示方式整合在一起，最终输出为一个数值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('env1')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc8c60f99e2e962c0e4a3e9c9f27c1bb5f2a586f6d03b97348d9b6648bd2cf92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
